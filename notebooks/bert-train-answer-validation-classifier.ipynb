{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install tf-models-official","execution_count":63,"outputs":[{"output_type":"stream","text":"Requirement already satisfied: tf-models-official in /opt/conda/lib/python3.7/site-packages (2.4.0)\nRequirement already satisfied: kaggle>=1.3.9 in /opt/conda/lib/python3.7/site-packages (from tf-models-official) (1.5.10)\nRequirement already satisfied: tensorflow-model-optimization>=0.4.1 in /opt/conda/lib/python3.7/site-packages (from tf-models-official) (0.5.0)\nRequirement already satisfied: sentencepiece in /opt/conda/lib/python3.7/site-packages (from tf-models-official) (0.1.95)\nRequirement already satisfied: opencv-python-headless in /opt/conda/lib/python3.7/site-packages (from tf-models-official) (4.5.1.48)\nRequirement already satisfied: dataclasses in /opt/conda/lib/python3.7/site-packages (from tf-models-official) (0.6)\nRequirement already satisfied: scipy>=0.19.1 in /opt/conda/lib/python3.7/site-packages (from tf-models-official) (1.5.4)\nRequirement already satisfied: google-cloud-bigquery>=0.31.0 in /opt/conda/lib/python3.7/site-packages (from tf-models-official) (1.12.1)\nRequirement already satisfied: tensorflow-datasets in /opt/conda/lib/python3.7/site-packages (from tf-models-official) (3.0.0)\nRequirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from tf-models-official) (1.15.0)\nRequirement already satisfied: tensorflow-hub>=0.6.0 in /opt/conda/lib/python3.7/site-packages (from tf-models-official) (0.11.0)\nRequirement already satisfied: psutil>=5.4.3 in /opt/conda/lib/python3.7/site-packages (from tf-models-official) (5.8.0)\nRequirement already satisfied: google-api-python-client>=1.6.7 in /opt/conda/lib/python3.7/site-packages (from tf-models-official) (1.8.0)\nRequirement already satisfied: Pillow in /opt/conda/lib/python3.7/site-packages (from tf-models-official) (7.2.0)\nRequirement already satisfied: oauth2client in /opt/conda/lib/python3.7/site-packages (from tf-models-official) (4.1.3)\nRequirement already satisfied: tensorflow>=2.4.0 in /opt/conda/lib/python3.7/site-packages (from tf-models-official) (2.4.1)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.7/site-packages (from tf-models-official) (5.3.1)\nRequirement already satisfied: tf-slim>=1.1.0 in /opt/conda/lib/python3.7/site-packages (from tf-models-official) (1.1.0)\nRequirement already satisfied: seqeval in /opt/conda/lib/python3.7/site-packages (from tf-models-official) (1.2.2)\nRequirement already satisfied: py-cpuinfo>=3.3.0 in /opt/conda/lib/python3.7/site-packages (from tf-models-official) (7.0.0)\nRequirement already satisfied: gin-config in /opt/conda/lib/python3.7/site-packages (from tf-models-official) (0.4.0)\nRequirement already satisfied: Cython in /opt/conda/lib/python3.7/site-packages (from tf-models-official) (0.29.21)\nRequirement already satisfied: numpy>=1.15.4 in /opt/conda/lib/python3.7/site-packages (from tf-models-official) (1.19.5)\nRequirement already satisfied: matplotlib in /opt/conda/lib/python3.7/site-packages (from tf-models-official) (3.3.3)\nRequirement already satisfied: pycocotools in /opt/conda/lib/python3.7/site-packages (from tf-models-official) (2.0.2)\nRequirement already satisfied: tensorflow-addons in /opt/conda/lib/python3.7/site-packages (from tf-models-official) (0.12.1)\nRequirement already satisfied: pandas>=0.22.0 in /opt/conda/lib/python3.7/site-packages (from tf-models-official) (1.2.0)\nRequirement already satisfied: google-auth>=1.4.1 in /opt/conda/lib/python3.7/site-packages (from google-api-python-client>=1.6.7->tf-models-official) (1.24.0)\nRequirement already satisfied: google-api-core<2dev,>=1.13.0 in /opt/conda/lib/python3.7/site-packages (from google-api-python-client>=1.6.7->tf-models-official) (1.22.4)\nRequirement already satisfied: uritemplate<4dev,>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from google-api-python-client>=1.6.7->tf-models-official) (3.0.1)\nRequirement already satisfied: google-auth-httplib2>=0.0.3 in /opt/conda/lib/python3.7/site-packages (from google-api-python-client>=1.6.7->tf-models-official) (0.0.4)\nRequirement already satisfied: httplib2<1dev,>=0.9.2 in /opt/conda/lib/python3.7/site-packages (from google-api-python-client>=1.6.7->tf-models-official) (0.18.1)\nRequirement already satisfied: protobuf>=3.12.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client>=1.6.7->tf-models-official) (3.14.0)\nRequirement already satisfied: requests<3.0.0dev,>=2.18.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client>=1.6.7->tf-models-official) (2.25.1)\nRequirement already satisfied: setuptools>=34.0.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client>=1.6.7->tf-models-official) (49.6.0.post20201009)\nRequirement already satisfied: pytz in /opt/conda/lib/python3.7/site-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client>=1.6.7->tf-models-official) (2020.5)\nRequirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client>=1.6.7->tf-models-official) (1.52.0)\nRequirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-auth>=1.4.1->google-api-python-client>=1.6.7->tf-models-official) (4.1.1)\nRequirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from google-auth>=1.4.1->google-api-python-client>=1.6.7->tf-models-official) (4.6)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth>=1.4.1->google-api-python-client>=1.6.7->tf-models-official) (0.2.7)\nRequirement already satisfied: google-cloud-core<2.0dev,>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery>=0.31.0->tf-models-official) (1.3.0)\nRequirement already satisfied: google-resumable-media>=0.3.1 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery>=0.31.0->tf-models-official) (1.2.0)\nRequirement already satisfied: google-crc32c<2.0dev,>=1.0 in /opt/conda/lib/python3.7/site-packages (from google-resumable-media>=0.3.1->google-cloud-bigquery>=0.31.0->tf-models-official) (1.1.0)\nRequirement already satisfied: cffi>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from google-crc32c<2.0dev,>=1.0->google-resumable-media>=0.3.1->google-cloud-bigquery>=0.31.0->tf-models-official) (1.14.4)\nRequirement already satisfied: pycparser in /opt/conda/lib/python3.7/site-packages (from cffi>=1.0.0->google-crc32c<2.0dev,>=1.0->google-resumable-media>=0.3.1->google-cloud-bigquery>=0.31.0->tf-models-official) (2.20)\nRequirement already satisfied: certifi in /opt/conda/lib/python3.7/site-packages (from kaggle>=1.3.9->tf-models-official) (2020.12.5)\nRequirement already satisfied: python-slugify in /opt/conda/lib/python3.7/site-packages (from kaggle>=1.3.9->tf-models-official) (4.0.1)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from kaggle>=1.3.9->tf-models-official) (4.55.1)\nRequirement already satisfied: python-dateutil in /opt/conda/lib/python3.7/site-packages (from kaggle>=1.3.9->tf-models-official) (2.8.1)\nRequirement already satisfied: urllib3 in /opt/conda/lib/python3.7/site-packages (from kaggle>=1.3.9->tf-models-official) (1.26.2)\nRequirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth>=1.4.1->google-api-python-client>=1.6.7->tf-models-official) (0.4.8)\nRequirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.13.0->google-api-python-client>=1.6.7->tf-models-official) (2.10)\nRequirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.13.0->google-api-python-client>=1.6.7->tf-models-official) (3.0.4)\nRequirement already satisfied: google-pasta~=0.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.4.0->tf-models-official) (0.2.0)\nRequirement already satisfied: termcolor~=1.1.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.4.0->tf-models-official) (1.1.0)\nRequirement already satisfied: grpcio~=1.32.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.4.0->tf-models-official) (1.32.0)\nRequirement already satisfied: flatbuffers~=1.12.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.4.0->tf-models-official) (1.12)\nRequirement already satisfied: wrapt~=1.12.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.4.0->tf-models-official) (1.12.1)\nRequirement already satisfied: h5py~=2.10.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.4.0->tf-models-official) (2.10.0)\nRequirement already satisfied: opt-einsum~=3.3.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.4.0->tf-models-official) (3.3.0)\nRequirement already satisfied: tensorboard~=2.4 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.4.0->tf-models-official) (2.4.1)\nRequirement already satisfied: wheel~=0.35 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.4.0->tf-models-official) (0.36.2)\nRequirement already satisfied: gast==0.3.3 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.4.0->tf-models-official) (0.3.3)\nRequirement already satisfied: astunparse~=1.6.3 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.4.0->tf-models-official) (1.6.3)\nRequirement already satisfied: absl-py~=0.10 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.4.0->tf-models-official) (0.10.0)\nRequirement already satisfied: tensorflow-estimator<2.5.0,>=2.4.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.4.0->tf-models-official) (2.4.0)\nRequirement already satisfied: keras-preprocessing~=1.1.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.4.0->tf-models-official) (1.1.2)\nRequirement already satisfied: typing-extensions~=3.7.4 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.4.0->tf-models-official) (3.7.4.3)\n","name":"stdout"},{"output_type":"stream","text":"Requirement already satisfied: werkzeug>=0.11.15 in /opt/conda/lib/python3.7/site-packages (from tensorboard~=2.4->tensorflow>=2.4.0->tf-models-official) (1.0.1)\nRequirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard~=2.4->tensorflow>=2.4.0->tf-models-official) (1.8.0)\nRequirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.7/site-packages (from tensorboard~=2.4->tensorflow>=2.4.0->tf-models-official) (3.3.3)\nRequirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /opt/conda/lib/python3.7/site-packages (from tensorboard~=2.4->tensorflow>=2.4.0->tf-models-official) (0.4.2)\nRequirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.7/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow>=2.4.0->tf-models-official) (1.3.0)\nRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from markdown>=2.6.8->tensorboard~=2.4->tensorflow>=2.4.0->tf-models-official) (3.3.0)\nRequirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow>=2.4.0->tf-models-official) (3.0.1)\nRequirement already satisfied: dm-tree~=0.1.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow-model-optimization>=0.4.1->tf-models-official) (0.1.5)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->markdown>=2.6.8->tensorboard~=2.4->tensorflow>=2.4.0->tf-models-official) (3.4.0)\nRequirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /opt/conda/lib/python3.7/site-packages (from matplotlib->tf-models-official) (2.4.7)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.7/site-packages (from matplotlib->tf-models-official) (0.10.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib->tf-models-official) (1.3.1)\nRequirement already satisfied: text-unidecode>=1.3 in /opt/conda/lib/python3.7/site-packages (from python-slugify->kaggle>=1.3.9->tf-models-official) (1.3)\nRequirement already satisfied: scikit-learn>=0.21.3 in /opt/conda/lib/python3.7/site-packages (from seqeval->tf-models-official) (0.24.1)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn>=0.21.3->seqeval->tf-models-official) (2.1.0)\nRequirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.7/site-packages (from scikit-learn>=0.21.3->seqeval->tf-models-official) (1.0.0)\nRequirement already satisfied: typeguard>=2.7 in /opt/conda/lib/python3.7/site-packages (from tensorflow-addons->tf-models-official) (2.11.1)\nRequirement already satisfied: promise in /opt/conda/lib/python3.7/site-packages (from tensorflow-datasets->tf-models-official) (2.3)\nRequirement already satisfied: tensorflow-metadata in /opt/conda/lib/python3.7/site-packages (from tensorflow-datasets->tf-models-official) (0.27.0)\nRequirement already satisfied: dill in /opt/conda/lib/python3.7/site-packages (from tensorflow-datasets->tf-models-official) (0.3.3)\nRequirement already satisfied: attrs>=18.1.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow-datasets->tf-models-official) (20.3.0)\nRequirement already satisfied: future in /opt/conda/lib/python3.7/site-packages (from tensorflow-datasets->tf-models-official) (0.18.2)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (4.55.1)\n","name":"stdout"}]},{"metadata":{"id":"Q4MCWngFbCe_","outputId":"7c7b6158-6d77-4da7-980a-38f5a4c18bbe","trusted":true},"cell_type":"code","source":"import tensorflow as tf\nimport numpy as np\n\nimport os\nimport random\nimport json\nimport gc\n# load BERT modules\nfrom official import nlp\nimport official.nlp.bert as bert\nimport official.nlp.bert.tokenization as tokenization\nimport official.nlp.bert.configs as configs\nimport official.nlp.bert.bert_models as bert_models\nimport official.nlp.optimization\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.utils import shuffle as shuffle_sklearn\nfrom sklearn.metrics import precision_score, recall_score, f1_score\n\nrandom_state = 42\n\nprint(f'Tensorflow version {tf.__version__}')\n# disable warning messages\ntf.get_logger().setLevel('ERROR')","execution_count":65,"outputs":[{"output_type":"stream","text":"Tensorflow version 2.4.1\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"gs_folder_bert = \"gs://cloud-tpu-checkpoints/bert/v3/uncased_L-12_H-768_A-12\"\ntf.io.gfile.listdir(gs_folder_bert)\nhub_url_bert = \"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/3\"","execution_count":3,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def json_load(name):\n    with open(f'{name}.json', 'r', encoding = 'utf-8') as f:\n        return json.load(f)\n    \ndef json_save(name, item):\n    with open(f'{name}.json', 'w', encoding = 'utf-8') as f:\n        json.dump(item, f, ensure_ascii = False, indent = 2)","execution_count":4,"outputs":[]},{"metadata":{"id":"eSnBrrAJIhB6","outputId":"a5ad2255-65c2-4618-fee6-0bb73e6e07cf","trusted":true},"cell_type":"code","source":"# set up tokenizer to generate Tensorflow dataset\ntokenizer = bert.tokenization.FullTokenizer(\n    vocab_file=os.path.join(gs_folder_bert, \"vocab.txt\"),\n     do_lower_case=True)\n\nprint(f'Vocab size: {len(tokenizer.vocab)}')","execution_count":5,"outputs":[{"output_type":"stream","text":"Vocab size: 30522\n","name":"stdout"}]},{"metadata":{"id":"kKd9iEyLe9jb","outputId":"049c03ff-518a-4eb6-ee54-2a7b5587e839","trusted":true},"cell_type":"code","source":"config_dict = {\n    'attention_probs_dropout_prob': 0.1,\n    'hidden_act': 'gelu',\n    'hidden_dropout_prob': 0.1,\n    'hidden_size': 768,\n    'initializer_range': 0.02,\n    'intermediate_size': 3072,\n    'max_position_embeddings': 512,\n    'num_attention_heads': 12,\n    'num_hidden_layers': 12,\n    'type_vocab_size': 2,\n    'vocab_size': 30522}\n\n\nbert_config_file = os.path.join(gs_folder_bert, \"bert_config.json\")\nconfig_dict = json.loads(tf.io.gfile.GFile(bert_config_file).read())\n\nbert_config = bert.configs.BertConfig.from_dict(config_dict)\n\nconfig_dict\n#bert_config = configs.BertConfig.from_dict(config_dict)","execution_count":6,"outputs":[{"output_type":"execute_result","execution_count":6,"data":{"text/plain":"{'attention_probs_dropout_prob': 0.1,\n 'hidden_act': 'gelu',\n 'hidden_dropout_prob': 0.1,\n 'hidden_size': 768,\n 'initializer_range': 0.02,\n 'intermediate_size': 3072,\n 'max_position_embeddings': 512,\n 'num_attention_heads': 12,\n 'num_hidden_layers': 12,\n 'type_vocab_size': 2,\n 'vocab_size': 30522}"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# convert sentence to tokens\ndef encode_sentence(s):\n    tokens = list(tokenizer.tokenize(s)) + ['[SEP]']\n    return tokenizer.convert_tokens_to_ids(tokens)\n\ndef encode_pair(q, a, max_size):\n    q_tok = ['[CLS]'] + tokenizer.tokenize(q) + ['[SEP]']\n    a_tok = tokenizer.tokenize(a) + ['[SEP]']\n    ids = tokenizer.convert_tokens_to_ids(q_tok + a_tok)\n    \n    if len(ids) > max_size:\n        raise IndexError('Too many tokens')\n    else:\n        inputs = {\n            'input_word_ids': ids + [0]*(max_size - len(ids)),\n            'input_mask': [1]*len(ids) + [0]*(max_size - len(ids)),\n            'input_type_ids': [0]*len(q_tok) + [1]*len(a_tok) + [0]*(max_size - len(ids))\n        }\n        \n        return inputs\n    \nassert(encode_sentence('Human is instance of animal') == [2529, 2003, 6013, 1997, 4111, 102])\nassert(\n    encode_pair('Who are you?', 'I am your dad.', 15)['input_word_ids'] ==\n    [101, 2040, 2024, 2017, 1029, 102, 1045, 2572, 2115, 3611, 1012, 102, 0, 0, 0]\n)","execution_count":7,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def bert_encode(class_0, class_1, tokenizer, size=0, random_state=42):\n    # random.shuffle(class_0[:size] if size else class_0)\n    # random.shuffle(class_1[:size] if size else class_1)\n    \n    labels = [0]*len(class_0) + [1]*len(class_1)\n    records = class_0 + class_1\n\n    questions = tf.ragged.constant([encode_sentence(s[0]) for s in records])\n    answers = tf.ragged.constant([encode_sentence(s[1]) for s in records])\n\n    cls = [tokenizer.convert_tokens_to_ids(['[CLS]'])]*questions.shape[0]\n    input_word_ids = tf.concat([cls, questions, answers], axis=-1)\n\n    input_mask = tf.ones_like(input_word_ids).to_tensor()\n\n    type_cls = tf.zeros_like(cls)\n    type_question = tf.zeros_like(questions)\n    type_answer = tf.ones_like(answers)\n    input_type_ids = tf.concat([type_cls, type_question, type_answer], axis=-1).to_tensor()\n\n    inputs = {\n        'input_word_ids': input_word_ids.to_tensor(),\n        'input_mask': input_mask,\n        'input_type_ids': input_type_ids}\n\n    return inputs, tf.convert_to_tensor(labels)","execution_count":8,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class DataGenerator(tf.keras.utils.Sequence):\n    'generates data batches'\n    def __init__(self, class_0, class_1, embed_len=180, batch_size=32, shuffle=True, random_state=42):\n        'Initialization'\n        self.text = class_0 + class_1\n        self.labels = [0]*len(class_0) + [1]*len(class_1)\n        self.embed_len = embed_len\n        self.batch_size = batch_size\n        self.shuffle = shuffle\n        self.count = len(self.text)\n        self.indexes = list(range(self.count))\n        self.data = [None]*len(self.text)\n        self.on_epoch_end()\n        \n        if self.shuffle:\n            self.indexes = shuffle_sklearn(self.indexes, random_state=random_state)\n\n    def __len__(self):\n        'Denotes the number of batches per epoch'\n        return self.count // self.batch_size\n\n    def __getitem__(self, index):\n        'Generate one batch of data'\n        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n\n        inputs = {\n            'input_word_ids': [],\n            'input_mask': [],\n            'input_type_ids': []}\n        \n        outputs = []\n        \n        for i in indexes:\n            if not self.data[i]:\n                self.data[i] = encode_pair(self.text[i][0], self.text[i][1], self.embed_len)\n            for key in inputs:\n                inputs[key] += [self.data[i][key]]\n            outputs.append(self.labels[i])\n            \n        for key in inputs:\n            inputs[key] = tf.ragged.constant(inputs[key], inner_shape=(self.batch_size, self.embed_len))\n        outputs = tf.convert_to_tensor(outputs)\n\n        return inputs, outputs\n    \n    def get_dataset(self):\n        inputs = {\n            'input_word_ids': [],\n            'input_mask': [],\n            'input_type_ids': []}\n        \n        outputs = []\n        \n        for i in range(len(self.text)):\n            if not self.data[i]:\n                self.data[i] = encode_pair(self.text[i][0], self.text[i][1], self.embed_len)\n            for key in inputs:\n                inputs[key] += [self.data[i][key]]\n            outputs.append(self.labels[i])\n            \n        return inputs, outputs","execution_count":9,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# story batch + epoch results and dump to file if name is not empty\nclass HistoryCallback(tf.keras.callbacks.Callback):\n\n    def __init__(self, file_name, history={'epoch': [], 'batch': []}):\n        self.history = history\n        self.name = file_name\n        self.epoch = None\n\n    def on_epoch_begin(self, epoch, logs=None):\n        self.epoch = epoch + 1\n        \n    def on_epoch_end(self, epoch, logs=None):\n        self.epoch = epoch + 1\n        logs['epoch'] = self.epoch\n        self.history['epoch'].append(logs)\n        \n        if self.name:\n            json_save(self.name, self.history)\n\n    def on_train_batch_end(self, batch, logs=None):\n        if logs and batch:\n            logs['batch'] = batch\n            logs['epoch'] = self.epoch\n            self.history['batch'].append(logs)","execution_count":10,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_train_set_samples_qald(data):\n    class_0 = list()\n    class_1 = list()\n\n    for i in range(len(data)):\n        true_answers = get_list_of_true_answers(qald_dataset[i]['answers']) # from original dataset\n\n        for j in range(len(qald[i]['answers'])): # data provided by AlGa\n            is_answer_correct = if_answer_correct(true_answers, qald[i]['answers'][j])\n            if is_answer_correct:\n                if len(data[str(i)][j]) > 0 and len(' '.join(data[str(i)][j]).split()) < 75:\n                    class_1.append((find_english_in_qald(qald_dataset[i]['question']), ' '.join(data[str(i)][j]))) # my data\n            else:\n                if len(data[str(i)][j]) > 0 and len(' '.join(data[str(i)][j]).split()) < 75:\n                    class_0.append((find_english_in_qald(qald_dataset[i]['question']), ' '.join(data[str(i)][j]))) # my data\n                # TODO: search for textual answer\n    \n    return class_0, class_1, 'labels_qald'","execution_count":11,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_qa_dataset_qald(data):\n    main = dict()\n\n    for i in range(len(data)):\n        true_answers = get_list_of_true_answers(qald_dataset[i]['answers']) # from original dataset\n        answers = list()\n        for j in range(len(qald[i]['answers'])): # data provided by AlGa\n            is_answer_correct = if_answer_correct(true_answers, qald[i]['answers'][j])\n            if is_answer_correct:\n                if len(data[str(i)][j]) > 0 and len(' '.join(data[str(i)][j]).split()) < 75:\n                    answers.append((1, (find_english_in_qald(qald_dataset[i]['question']), ' '.join(data[str(i)][j])))) # my data\n            else:\n                if len(data[str(i)][j]) > 0 and len(' '.join(data[str(i)][j]).split()) < 75:\n                    answers.append((0, (find_english_in_qald(qald_dataset[i]['question']), ' '.join(data[str(i)][j])))) # my data\n                # TODO: search for textual answer\n            \n        main[i] = answers\n    \n    return main","execution_count":38,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!ls ../input/answer-validation","execution_count":13,"outputs":[{"output_type":"stream","text":"qald-9-Dbpedia.json\t       qald-9-train-multilingual.json\r\nqald-9-test-multilingual.json  qald_labels.json\r\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"qald = json_load(\"../input/answer-validation/qald-9-Dbpedia\")\ndata = json_load(\"../input/answer-validation/qald_labels\")\n\nqald_test = json_load(\"../input/answer-validation/qald-9-test-multilingual\")\nqald_train = json_load(\"../input/answer-validation/qald-9-train-multilingual\")\n\nqald_dataset = qald_train['questions'] + qald_test['questions']","execution_count":14,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def find_english_in_qald(representations):\n    \"\"\"\n    representations: qald_dataset[i]['question']\n    \"\"\"\n    for r in representations:\n        if r['language'] == 'en':\n            return r['string']\n    \n    assert False","execution_count":15,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_list_of_true_answers(qald_answers):\n    \"\"\"\n    qald_answers: qald_train['questions'][0]['answers']\n    \"\"\"\n    true = list()\n    if 'bindings' in qald_answers[0]['results']:\n        for bind in qald_answers[0]['results']['bindings']:\n            k = list(bind.keys())[0]\n            true.append(bind[k]['value'])\n    \n    return true","execution_count":16,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def if_answer_correct(true_uri, pred_answer):\n    \"\"\"\n    pred_answers: qald[i]['answers'][j]\n    \"\"\"\n    uris = list()\n    if pred_answer['DBpedia']:\n        for triple in pred_answer['DBpedia']:\n            for k in list(triple.keys()):\n                if 'p' not in k and 'dbpedia' in triple[k]['value']:\n                    uris.append(triple[k]['value'])\n    # answer_uris.append(uris)\n    # print(true_uri)\n    if any(true in uris for true in true_uri):\n        return True\n    else:\n        return False","execution_count":17,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# quick consistence check\nfor i in range(len(qald)):\n    assert len(qald[i]['answers']) == len(data[str(i)])","execution_count":18,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"main = create_qa_dataset_qald(data)\njson_save(\"qald-qa-dataset\", main)","execution_count":39,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class_0, class_1, dataset_name = create_train_set_samples_qald(data)\nclass_0 = random.sample(class_0, int(len(class_0)/2))\nprint(dataset_name)\nprint(len(class_0))\nprint(len(class_1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class_1[50], class_0[50]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_model(bert_classifier, epochs=100, batch_size=8, batches_per_epoch=1000, warmup_epochs=5):\n    num_train_steps = epochs*batch_size*batches_per_epoch\n    warmup_steps = batches_per_epoch*warmup_epochs\n\n    # creates an optimizer with learning rate schedule\n    optimizer = nlp.optimization.create_optimizer(\n        2e-5, num_train_steps=num_train_steps, num_warmup_steps=warmup_steps)\n\n    metrics = [tf.keras.metrics.SparseCategoricalAccuracy('accuracy', dtype=tf.float32)]\n    loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n\n    bert_classifier.compile(\n        optimizer=optimizer,\n        loss=loss,\n        metrics=metrics)\n    \n    # bert_classifier.load_weights(os.path.join(*file_pretrained))\n    \n    return bert_classifier","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_model(bert_classifier, \n                class_0, class_1, \n                # valid_0, valid_1, \n                name,\n                epochs=5, \n                batch_size=8, \n                warmup_epochs=5,\n                embed=96,\n                rand_idx=0 # use different random index to shuffle training data\n               ):\n    \n    \n    rs = list(range(2010, 2020)) # generate random states\n    \n    # valid_set, valid_labels = bert_encode(valid_0, valid_1, tokenizer) \n    train = DataGenerator(class_0, class_1, embed_len=embed, batch_size=batch_size, random_state=rs[rand_idx])\n    n_steps = len(class_0 + class_1) // batch_size # define number of steps per epoch\n    \n    history = HistoryCallback(file_name=name, history={'epoch': [], 'batch': []})\n    \n    bert_classifier.fit(\n        train,\n        steps_per_epoch=n_steps,\n        # validation_data=(valid_set, valid_labels),\n        batch_size=batch_size,\n        epochs=epochs,\n        callbacks=[\n            history,\n            tf.keras.callbacks.EarlyStopping(monitor='val_accuracy',\n                                             mode='max',\n                                             patience=1,\n                                             restore_best_weights=True)\n        ],\n    )    ","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"train_0, test_0 = train_test_split(class_0, test_size=0.33, random_state=4)\ntrain_1, test_1 = train_test_split(class_1, test_size=0.33, random_state=4)\n\ntest_set, test_labels = bert_encode(test_0, test_1, tokenizer)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gc.collect()\n\nbatch_size = 16\nembed = 256\n\nbert_classifier, bert_encoder = bert_models.classifier_model(bert_config, num_labels=2)\nbert_classifier = create_model(bert_classifier, batch_size=8)\n\ntrain_model(bert_classifier, \n            train_0, train_1,\n            # valid_0, valid_1, \n            'generated.json',\n            batch_size=batch_size, embed=embed, rand_idx=0)\n\ny_pred = bert_classifier.predict(test_set, verbose=1)\ny_pred = np.argmax(y_pred, axis=1)\ny_true = test_labels.numpy()\n\nprint(\"Precision\", precision_score(y_true, y_pred))\nprint(\"Recall\", recall_score(y_true, y_pred))\nprint(\"F1 Score\", f1_score(y_true, y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tf.saved_model.save(bert_classifier, export_dir='./')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Testing currently deployed models"},{"metadata":{"trusted":true},"cell_type":"code","source":"import requests\n\ntest_set, test_labels = bert_encode(test_0, test_1, tokenizer)\n\ntest_set['input_word_ids'] = test_set['input_word_ids'].numpy().tolist()\ntest_set['input_type_ids'] = test_set['input_type_ids'].numpy().tolist()\ntest_set['input_mask'] = test_set['input_mask'].numpy().tolist()\n\nheaders = {\"content-type\": \"application/json\"}\ndata = json.dumps({\"signature_name\": \"serving_default\", \"inputs\": test_set})\n\njson_response = requests.post('http://webengineering.ins.hs-anhalt.de:41002/v1/models/answer_validation:predict',\n                              data=data,\n                              headers=headers)","execution_count":51,"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'test_0' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-51-4ffe5bb95f93>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtest_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbert_encode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_0\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtest_set\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_word_ids'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_set\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_word_ids'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'test_0' is not defined"]}]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = np.argmax(json_response.json()['outputs'], axis=1)\ny_true = test_labels.numpy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Precision\", precision_score(y_true, y_pred))\nprint(\"Recall\", recall_score(y_true, y_pred))\nprint(\"F1 Score\", f1_score(y_true, y_pred))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.9","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}