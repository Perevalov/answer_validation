{"metadata":{"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install tf-models-official","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nimport numpy as np\n\nimport os\nimport random\nimport json\nimport gc\n# load BERT modules\nfrom official import nlp\nimport official.nlp.bert as bert\nimport official.nlp.bert.tokenization as tokenization\nimport official.nlp.bert.configs as configs\nimport official.nlp.bert.bert_models as bert_models\nimport official.nlp.optimization\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.utils import shuffle as shuffle_sklearn\nfrom sklearn.metrics import precision_score, recall_score, f1_score\n\nrandom_state = 42\n\nprint(f'Tensorflow version {tf.__version__}')\n# disable warning messages\ntf.get_logger().setLevel('ERROR')","metadata":{"id":"Q4MCWngFbCe_","outputId":"7c7b6158-6d77-4da7-980a-38f5a4c18bbe","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gs_folder_bert = \"gs://cloud-tpu-checkpoints/bert/v3/uncased_L-12_H-768_A-12\"\ntf.io.gfile.listdir(gs_folder_bert)\nhub_url_bert = \"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/3\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def json_load(name):\n    with open(f'{name}.json', 'r', encoding = 'utf-8') as f:\n        return json.load(f)\n    \ndef json_save(name, item):\n    with open(f'{name}.json', 'w', encoding = 'utf-8') as f:\n        json.dump(item, f, ensure_ascii = False, indent = 2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# set up tokenizer to generate Tensorflow dataset\ntokenizer = bert.tokenization.FullTokenizer(\n    vocab_file=os.path.join(gs_folder_bert, \"vocab.txt\"),\n     do_lower_case=True)\n\nprint(f'Vocab size: {len(tokenizer.vocab)}')","metadata":{"id":"eSnBrrAJIhB6","outputId":"a5ad2255-65c2-4618-fee6-0bb73e6e07cf","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"config_dict = {\n    'attention_probs_dropout_prob': 0.1,\n    'hidden_act': 'gelu',\n    'hidden_dropout_prob': 0.1,\n    'hidden_size': 768,\n    'initializer_range': 0.02,\n    'intermediate_size': 3072,\n    'max_position_embeddings': 512,\n    'num_attention_heads': 12,\n    'num_hidden_layers': 12,\n    'type_vocab_size': 2,\n    'vocab_size': 30522}\n\n\nbert_config_file = os.path.join(gs_folder_bert, \"bert_config.json\")\nconfig_dict = json.loads(tf.io.gfile.GFile(bert_config_file).read())\n\nbert_config = bert.configs.BertConfig.from_dict(config_dict)\n\nconfig_dict\n#bert_config = configs.BertConfig.from_dict(config_dict)","metadata":{"id":"kKd9iEyLe9jb","outputId":"049c03ff-518a-4eb6-ee54-2a7b5587e839","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# convert sentence to tokens\ndef encode_sentence(s):\n    tokens = list(tokenizer.tokenize(s))[:256] + ['[SEP]']\n    return tokenizer.convert_tokens_to_ids(tokens)\n\ndef encode_pair(q, a, max_size):\n    q_tok = ['[CLS]'] + tokenizer.tokenize(q)[:int(max_size/2)-1] + ['[SEP]']\n    a_tok = tokenizer.tokenize(a)[:int(max_size/2)-1] + ['[SEP]']\n    ids = tokenizer.convert_tokens_to_ids(q_tok + a_tok)\n    \n    if len(ids) > max_size:\n        raise IndexError('Too many tokens: max={0}, actual={1}'.format(len(ids), max_size))\n    else:\n        inputs = {\n            'input_word_ids': ids + [0]*(max_size - len(ids)),\n            'input_mask': [1]*len(ids) + [0]*(max_size - len(ids)),\n            'input_type_ids': [0]*len(q_tok) + [1]*len(a_tok) + [0]*(max_size - len(ids))\n        }\n        \n        return inputs\n    \nassert(encode_sentence('Human is instance of animal') == [2529, 2003, 6013, 1997, 4111, 102])\nassert(\n    encode_pair('Who are you?', 'I am your dad.', 15)['input_word_ids'] ==\n    [101, 2040, 2024, 2017, 1029, 102, 1045, 2572, 2115, 3611, 1012, 102, 0, 0, 0]\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def bert_encode(class_0, class_1, tokenizer, size=0, random_state=42):\n    # random.shuffle(class_0[:size] if size else class_0)\n    # random.shuffle(class_1[:size] if size else class_1)\n    \n    labels = [0]*len(class_0) + [1]*len(class_1)\n    records = class_0 + class_1\n\n    questions = tf.ragged.constant([encode_sentence(s[0]) for s in records])\n    answers = tf.ragged.constant([encode_sentence(s[1]) for s in records])\n\n    cls = [tokenizer.convert_tokens_to_ids(['[CLS]'])]*questions.shape[0]\n    input_word_ids = tf.concat([cls, questions, answers], axis=-1)\n\n    input_mask = tf.ones_like(input_word_ids).to_tensor()\n\n    type_cls = tf.zeros_like(cls)\n    type_question = tf.zeros_like(questions)\n    type_answer = tf.ones_like(answers)\n    input_type_ids = tf.concat([type_cls, type_question, type_answer], axis=-1).to_tensor()\n\n    inputs = {\n        'input_word_ids': input_word_ids.to_tensor(),\n        'input_mask': input_mask,\n        'input_type_ids': input_type_ids}\n\n    return inputs, tf.convert_to_tensor(labels)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class DataGenerator(tf.keras.utils.Sequence):\n    'generates data batches'\n    def __init__(self, class_0, class_1, embed_len=512, batch_size=32, shuffle=True, random_state=42):\n        'Initialization'\n        self.text = class_0 + class_1\n        self.labels = [0]*len(class_0) + [1]*len(class_1)\n        self.embed_len = embed_len\n        self.batch_size = batch_size\n        self.shuffle = shuffle\n        self.count = len(self.text)\n        self.indexes = list(range(self.count))\n        self.data = [None]*len(self.text)\n        self.on_epoch_end()\n        \n        if self.shuffle:\n            self.indexes = shuffle_sklearn(self.indexes, random_state=random_state)\n\n    def __len__(self):\n        'Denotes the number of batches per epoch'\n        return self.count // self.batch_size\n\n    def __getitem__(self, index):\n        'Generate one batch of data'\n        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n\n        inputs = {\n            'input_word_ids': [],\n            'input_mask': [],\n            'input_type_ids': []}\n        \n        outputs = []\n        \n        for i in indexes:\n            if not self.data[i]:\n                self.data[i] = encode_pair(self.text[i][0], self.text[i][1], self.embed_len)\n            for key in inputs:\n                inputs[key] += [self.data[i][key]]\n            outputs.append(self.labels[i])\n            \n        for key in inputs:\n            inputs[key] = tf.ragged.constant(inputs[key], inner_shape=(self.batch_size, self.embed_len))\n        outputs = tf.convert_to_tensor(outputs)\n\n        return inputs, outputs\n    \n    def get_dataset(self):\n        inputs = {\n            'input_word_ids': [],\n            'input_mask': [],\n            'input_type_ids': []}\n        \n        outputs = []\n        \n        for i in range(len(self.text)):\n            if not self.data[i]:\n                self.data[i] = encode_pair(self.text[i][0], self.text[i][1], self.embed_len)\n            for key in inputs:\n                inputs[key] += [self.data[i][key]]\n            outputs.append(self.labels[i])\n            \n        return inputs, outputs","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# story batch + epoch results and dump to file if name is not empty\nclass HistoryCallback(tf.keras.callbacks.Callback):\n\n    def __init__(self, file_name, history={'epoch': [], 'batch': []}):\n        self.history = history\n        self.name = file_name\n        self.epoch = None\n\n    def on_epoch_begin(self, epoch, logs=None):\n        self.epoch = epoch + 1\n        \n    def on_epoch_end(self, epoch, logs=None):\n        self.epoch = epoch + 1\n        logs['epoch'] = self.epoch\n        self.history['epoch'].append(logs)\n        \n        if self.name:\n            json_save(self.name, self.history)\n\n    def on_train_batch_end(self, batch, logs=None):\n        if logs and batch:\n            logs['batch'] = batch\n            logs['epoch'] = self.epoch\n            self.history['batch'].append(logs)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_train_set_samples_qald(data):\n    class_0 = list()\n    class_1 = list()\n\n    for i in range(len(data)):\n        true_answers = get_list_of_true_answers(qald_dataset[i]['answers']) # from original dataset\n\n        for j in range(len(qald[i]['answers'])): # data provided by AlGa\n            is_answer_correct = if_answer_correct(true_answers, qald[i]['answers'][j])\n            if is_answer_correct:\n                if len(data[str(i)][j]) > 0 and len(' '.join(data[str(i)][j]).split()):\n                    class_1.append((find_english_in_qald(qald_dataset[i]['question']), ' '.join(data[str(i)][j]))) # my data\n            else:\n                if len(data[str(i)][j]) > 0 and len(' '.join(data[str(i)][j]).split()):\n                    class_0.append((find_english_in_qald(qald_dataset[i]['question']), ' '.join(data[str(i)][j]))) # my data\n                # TODO: search for textual answer\n    \n    return class_0, class_1, 'labels_qald'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_train_set_samples_qald_wikidata(labels, answers):\n    class_0 = list()\n    class_1 = list()\n\n    for i in range(len(answers)):\n        for j in range(len(answers[i]['response'])): # data provided by AlGa\n            is_answer_correct = answers[i]['response'][j]['is_true']\n            if is_answer_correct:\n                if len(labels[i]['responses'][j]) > 0:\n                    class_1.append((wikidata_qald[i]['question_text'], ' '.join(labels[i]['responses'][j]))) # my data\n            else:\n                if len(labels[i]['responses'][j]) > 0:\n                    class_0.append((wikidata_qald[i]['question_text'], ' '.join(labels[i]['responses'][j]))) # my data\n                # TODO: search for textual answer\n    \n    return class_0, class_1, 'labels_qald'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_qa_dataset_qald(data):\n    tmp = list()\n    main = dict()\n\n    for i in range(len(data.keys())):\n        keylist = list(data.keys())\n        true_answers = get_list_of_true_answers(qald_dataset[i]['answers']) # from original dataset\n        answers = list()\n        for j in range(len(qald[i]['answers'])): # data provided by AlGa\n            is_answer_correct = if_answer_correct(true_answers, qald[i]['answers'][j])\n            if is_answer_correct:\n                if len(data[keylist[i]][j]) > 0 and len(' '.join(data[keylist[i]][j]).split()) < 75:\n                    answers.append((1, (find_english_in_qald(qald_dataset[i]['question']), ' '.join(data[keylist[i]][j])))) # my data\n            else:\n                if len(data[keylist[i]][j]) > 0 and len(' '.join(data[keylist[i]][j]).split()) < 75:\n                    answers.append((0, (find_english_in_qald(qald_dataset[i]['question']), ' '.join(data[keylist[i]][j])))) # my data\n                # TODO: search for textual answer\n                # tmp.append({'question': find_english_in_qald(qald_dataset[i]['question']), 'query': qald[i]['answers'][j]['SPARQL'],'true': true_answers, 'pred': uris})\n        main[i] = answers\n    \n    return main, tmp","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!ls ../input/answer-validation","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"qald = json_load(\"../input/answer-validation/qald-9-Dbpedia\")\ndata = json_load(\"../input/answer-validation/qald_labels\")\n\nqald_test = json_load(\"../input/answer-validation/qald-9-test-multilingual\")\nqald_train = json_load(\"../input/answer-validation/qald-9-train-multilingual\")\n\nqald_dataset = qald_train['questions'] # + qald_test['questions']\ndata = {k: data[k] for k in list(data.keys())[:len(qald_train['questions'])]}\nqald = qald[:len(qald_train['questions'])]\n\nwikidata_labels = json_load(\"../input/answer-validation/qanswer_test_responses_labels\")\nwikidata_answers = json_load(\"../input/answer-validation/qanswer_test_responses_extended\")\nwikidata_qald = json_load(\"../input/answer-validation/qald_test_wdt\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def find_english_in_qald(representations):\n    \"\"\"\n    representations: qald_dataset[i]['question']\n    \"\"\"\n    for r in representations:\n        if r['language'] == 'en':\n            return r['string']\n    \n    assert False","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_list_of_true_answers(qald_answers):\n    \"\"\"\n    qald_answers: qald_train['questions'][0]['answers']\n    \"\"\"\n    true = list()\n    if 'bindings' in qald_answers[0]['results']:\n        for bind in qald_answers[0]['results']['bindings']:\n            k = list(bind.keys())[0]\n            true.append(bind[k]['value'])\n    \n    return true","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def if_answer_correct(true_uri, pred_answer):\n    \"\"\"\n    pred_answers: qald[i]['answers'][j]\n    \"\"\"\n    uris = list()\n    if pred_answer['DBpedia']:\n        for triple in pred_answer['DBpedia']:\n            for k in list(triple.keys()):\n                if 'p' not in k and 'dbpedia' in triple[k]['value']:\n                    if triple[k]['value'] not in uris:\n                        uris.append(triple[k]['value'])\n                    \n    # answer_uris.append(uris)\n    # print(true_uri)\n    \n    # cnt = 0\n    # for true in true_uri:\n    #    if true in uris:\n    #        cnt+1\n            \n    # if len(true_uri) > 0 and cnt < len(true_uri) and any(true in uris for true in true_uri):\n    #    if len(uris) > 0 and len(true_uri) != len(uris):\n    #        return False, uris\n    #    else:\n    #        return True, uris\n    # else:\n    #    return True, uris\n    \n    cnt = 0\n    for pred in uris:\n        if pred in true_uri:\n            cnt +=1\n    if len(uris) > 0 and cnt/len(uris) > 0.5: # precision > 0.5\n        return True\n    else:\n        return False","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# quick consistence check\nfor i in range(len(qald)):\n    assert len(qald[i]['answers']) == len(data[str(i)])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class_0, class_1, dataset_name = create_train_set_samples_qald_wikidata(wikidata_labels, wikidata_answers)\nclass_0 = random.sample(class_0, int(len(class_1)))\nprint(dataset_name)\nprint(len(class_0))\nprint(len(class_1))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"qald = json_load(\"../input/answer-validation/qald-9-Dbpedia\")\ndata = json_load(\"../input/answer-validation/qald_labels\")\n\ndata = {k: data[k] for k in list(data.keys())[len(qald_train['questions']):]}\nqald_dataset = qald_test['questions']\nqald = qald[len(qald_train['questions']):]\n\nmain = create_qa_dataset_qald(data)\njson_save(\"main\", main)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class_1[50], class_0[50]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_model(bert_classifier, epochs=100, batch_size=8, batches_per_epoch=1000, warmup_epochs=5):\n    num_train_steps = epochs*batch_size*batches_per_epoch\n    warmup_steps = batches_per_epoch*warmup_epochs\n\n    # creates an optimizer with learning rate schedule\n    optimizer = nlp.optimization.create_optimizer(\n        2e-5, num_train_steps=num_train_steps, num_warmup_steps=warmup_steps)\n\n    metrics = [tf.keras.metrics.SparseCategoricalAccuracy('accuracy', dtype=tf.float32)]\n    loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n\n    bert_classifier.compile(\n        optimizer=optimizer,\n        loss=loss,\n        metrics=metrics)\n    \n    # bert_classifier.load_weights(os.path.join(*file_pretrained))\n    \n    return bert_classifier","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_model(bert_classifier, \n                class_0, class_1, \n                # valid_0, valid_1, \n                name,\n                epochs=5, \n                batch_size=8, \n                warmup_epochs=5,\n                embed=256,\n                rand_idx=0 # use different random index to shuffle training data\n               ):\n    \n    \n    rs = list(range(2010, 2020)) # generate random states\n    \n    # valid_set, valid_labels = bert_encode(valid_0, valid_1, tokenizer) \n    train = DataGenerator(class_0, class_1, embed_len=embed, batch_size=batch_size, random_state=rs[rand_idx])\n    n_steps = len(class_0 + class_1) // batch_size # define number of steps per epoch\n    \n    history = HistoryCallback(file_name=name, history={'epoch': [], 'batch': []})\n    \n    bert_classifier.fit(\n        train,\n        steps_per_epoch=n_steps,\n        # validation_data=(valid_set, valid_labels),\n        batch_size=batch_size,\n        epochs=epochs,\n        callbacks=[\n            history,\n            tf.keras.callbacks.EarlyStopping(monitor='val_accuracy',\n                                             mode='max',\n                                             patience=1,\n                                             restore_best_weights=True)\n        ],\n    )    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_0, test_0 = train_test_split(class_0, test_size=0.1, random_state=4)\ntrain_1, test_1 = train_test_split(class_1, test_size=0.1, random_state=4)\n\ntest_set, test_labels = bert_encode(test_0, test_1, tokenizer)","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gc.collect()\n\nbatch_size = 16\nembed = 256\n\nbert_classifier, bert_encoder = bert_models.classifier_model(bert_config, num_labels=2)\nbert_classifier = create_model(bert_classifier, batch_size=8)\n\ntrain_model(bert_classifier, \n            class_0, class_1,\n            # valid_0, valid_1, \n            'generated.json',\n            batch_size=batch_size, embed=embed, rand_idx=0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred = bert_classifier.predict(test_set, verbose=1)\ny_pred = np.argmax(y_pred, axis=1)\ny_true = test_labels.numpy()\n\nprint(\"Precision\", precision_score(y_true, y_pred))\nprint(\"Recall\", recall_score(y_true, y_pred))\nprint(\"F1 Score\", f1_score(y_true, y_pred))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_set.keys()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tf.saved_model.save(bert_classifier, export_dir='./')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Testing currently deployed models","metadata":{}},{"cell_type":"code","source":"import requests\n\ntest_set, test_labels = bert_encode(test_0, test_1, tokenizer)\n\ntest_set['input_word_ids'] = test_set['input_word_ids'].numpy().tolist()\ntest_set['input_type_ids'] = test_set['input_type_ids'].numpy().tolist()\ntest_set['input_mask'] = test_set['input_mask'].numpy().tolist()\n\nheaders = {\"content-type\": \"application/json\"}\ndata = json.dumps({\"signature_name\": \"serving_default\", \"inputs\": test_set})\n\njson_response = requests.post('http://webengineering.ins.hs-anhalt.de:41002/v1/models/answer_validation:predict',\n                              data=data,\n                              headers=headers)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred = np.argmax(json_response.json()['outputs'], axis=1)\ny_true = test_labels.numpy()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Precision\", precision_score(y_true, y_pred))\nprint(\"Recall\", recall_score(y_true, y_pred))\nprint(\"F1 Score\", f1_score(y_true, y_pred))","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}